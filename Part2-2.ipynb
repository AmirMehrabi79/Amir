{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: `Amirmohammad Mehrabi` \n",
    "\n",
    "Student ID: `98102419`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = pd.read_csv('D:\\Data Science\\Project\\Data\\\\training.1600000.processed.noemoticon.csv', encoding='latin1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0           1                             2         3  \\\n",
       "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599995  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                       4                                                  5  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Adding Columns Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify column names\n",
    "column_names = ['Label', 'ID', 'Date', 'Unknown', 'Username', 'Tweet']\n",
    "\n",
    "# Assign the new column names\n",
    "p_data.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label          ID                          Date   Unknown  \\\n",
       "0            0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1            0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2            0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3            0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4            0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "1599995      4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996      4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997      4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998      4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999      4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                Username                                              Tweet  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Ms_Hip_Hop im glad ur doing well '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data['Tweet'].iloc[1599989]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NO_QUERY'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data['Unknown'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label          ID                          Date   Unknown  \\\n",
       "0            0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1            0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2            0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3            0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4            0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "1599995      4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996      4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997      4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998      4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999      4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                Username                                              Tweet  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicates\n",
    "\n",
    "p_data.drop_duplicates(inplace=True)\n",
    "p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label          ID                          Date         Username  \\\n",
       "0            0  1467810369  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_   \n",
       "1            0  1467810672  Mon Apr 06 22:19:49 PDT 2009    scotthamilton   \n",
       "2            0  1467810917  Mon Apr 06 22:19:53 PDT 2009         mattycus   \n",
       "3            0  1467811184  Mon Apr 06 22:19:57 PDT 2009          ElleCTF   \n",
       "4            0  1467811193  Mon Apr 06 22:19:57 PDT 2009           Karoli   \n",
       "...        ...         ...                           ...              ...   \n",
       "1599995      4  2193601966  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028   \n",
       "1599996      4  2193601969  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards   \n",
       "1599997      4  2193601991  Tue Jun 16 08:40:49 PDT 2009           bpbabe   \n",
       "1599998      4  2193602064  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz   \n",
       "1599999      4  2193602129  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris   \n",
       "\n",
       "                                                     Tweet  \n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1        is upset that he can't update his Facebook by ...  \n",
       "2        @Kenichan I dived many times for the ball. Man...  \n",
       "3          my whole body feels itchy and like its on fire   \n",
       "4        @nationwideclass no, it's not behaving at all....  \n",
       "...                                                    ...  \n",
       "1599995  Just woke up. Having no school is the best fee...  \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing useless Column\n",
    "\n",
    "p_data.drop(columns=['Unknown'], inplace=True)\n",
    "p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>just woke up. having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>thewdb.com - very cool to hear old walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>are you ready for your mojo makeover? ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>happy 38th birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @thenspcc @sparkscharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label          ID                          Date         Username  \\\n",
       "0            0  1467810369  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_   \n",
       "1            0  1467810672  Mon Apr 06 22:19:49 PDT 2009    scotthamilton   \n",
       "2            0  1467810917  Mon Apr 06 22:19:53 PDT 2009         mattycus   \n",
       "3            0  1467811184  Mon Apr 06 22:19:57 PDT 2009          ElleCTF   \n",
       "4            0  1467811193  Mon Apr 06 22:19:57 PDT 2009           Karoli   \n",
       "...        ...         ...                           ...              ...   \n",
       "1599995      4  2193601966  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028   \n",
       "1599996      4  2193601969  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards   \n",
       "1599997      4  2193601991  Tue Jun 16 08:40:49 PDT 2009           bpbabe   \n",
       "1599998      4  2193602064  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz   \n",
       "1599999      4  2193602129  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris   \n",
       "\n",
       "                                                     Tweet  \n",
       "0        @switchfoot http://twitpic.com/2y1zl - awww, t...  \n",
       "1        is upset that he can't update his facebook by ...  \n",
       "2        @kenichan i dived many times for the ball. man...  \n",
       "3          my whole body feels itchy and like its on fire   \n",
       "4        @nationwideclass no, it's not behaving at all....  \n",
       "...                                                    ...  \n",
       "1599995  just woke up. having no school is the best fee...  \n",
       "1599996  thewdb.com - very cool to hear old walt interv...  \n",
       "1599997  are you ready for your mojo makeover? ask me f...  \n",
       "1599998  happy 38th birthday to my boo of alll time!!! ...  \n",
       "1599999  happy #charitytuesday @thenspcc @sparkscharity...  \n",
       "\n",
       "[1600000 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower case\n",
    "\n",
    "p_data['Tweet'] = p_data['Tweet'].str.lower()\n",
    "p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>- awww, that's a bummer.  you shoulda got da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>i dived many times for the ball. managed to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>just woke up. having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>thewdb.com - very cool to hear old walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>are you ready for your mojo makeover? ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>happy 38th birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label          ID                          Date         Username  \\\n",
       "0            0  1467810369  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_   \n",
       "1            0  1467810672  Mon Apr 06 22:19:49 PDT 2009    scotthamilton   \n",
       "2            0  1467810917  Mon Apr 06 22:19:53 PDT 2009         mattycus   \n",
       "3            0  1467811184  Mon Apr 06 22:19:57 PDT 2009          ElleCTF   \n",
       "4            0  1467811193  Mon Apr 06 22:19:57 PDT 2009           Karoli   \n",
       "...        ...         ...                           ...              ...   \n",
       "1599995      4  2193601966  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028   \n",
       "1599996      4  2193601969  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards   \n",
       "1599997      4  2193601991  Tue Jun 16 08:40:49 PDT 2009           bpbabe   \n",
       "1599998      4  2193602064  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz   \n",
       "1599999      4  2193602129  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris   \n",
       "\n",
       "                                                     Tweet  \n",
       "0          - awww, that's a bummer.  you shoulda got da...  \n",
       "1        is upset that he can't update his facebook by ...  \n",
       "2         i dived many times for the ball. managed to s...  \n",
       "3          my whole body feels itchy and like its on fire   \n",
       "4         no, it's not behaving at all. i'm mad. why am...  \n",
       "...                                                    ...  \n",
       "1599995  just woke up. having no school is the best fee...  \n",
       "1599996  thewdb.com - very cool to hear old walt interv...  \n",
       "1599997  are you ready for your mojo makeover? ask me f...  \n",
       "1599998  happy 38th birthday to my boo of alll time!!! ...  \n",
       "1599999                          happy #charitytuesday      \n",
       "\n",
       "[1600000 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text cleaning\n",
    "\n",
    "import re\n",
    "\n",
    "p_data['Tweet'] = p_data['Tweet'].apply(lambda x: re.sub(r'http\\S+|@\\S+', '', x))\n",
    "p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>- awww, that's a bummer.  you shoulda got da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>i dived many times for the ball. managed to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>just woke up. having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>thewdb.com - very cool to hear old walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>are you ready for your mojo makeover? ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>happy 38th birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label          ID                          Date         Username  \\\n",
       "0            0  1467810369  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_   \n",
       "1            0  1467810672  Mon Apr 06 22:19:49 PDT 2009    scotthamilton   \n",
       "2            0  1467810917  Mon Apr 06 22:19:53 PDT 2009         mattycus   \n",
       "3            0  1467811184  Mon Apr 06 22:19:57 PDT 2009          ElleCTF   \n",
       "4            0  1467811193  Mon Apr 06 22:19:57 PDT 2009           Karoli   \n",
       "...        ...         ...                           ...              ...   \n",
       "1599995      4  2193601966  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028   \n",
       "1599996      4  2193601969  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards   \n",
       "1599997      4  2193601991  Tue Jun 16 08:40:49 PDT 2009           bpbabe   \n",
       "1599998      4  2193602064  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz   \n",
       "1599999      4  2193602129  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris   \n",
       "\n",
       "                                                     Tweet  \n",
       "0          - awww, that's a bummer.  you shoulda got da...  \n",
       "1        is upset that he can't update his facebook by ...  \n",
       "2         i dived many times for the ball. managed to s...  \n",
       "3          my whole body feels itchy and like its on fire   \n",
       "4         no, it's not behaving at all. i'm mad. why am...  \n",
       "...                                                    ...  \n",
       "1599995  just woke up. having no school is the best fee...  \n",
       "1599996  thewdb.com - very cool to hear old walt interv...  \n",
       "1599997  are you ready for your mojo makeover? ask me f...  \n",
       "1599998  happy 38th birthday to my boo of alll time!!! ...  \n",
       "1599999                          happy #charitytuesday      \n",
       "\n",
       "[1600000 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data.reset_index(drop=True, inplace=True)\n",
    "p_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Building a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Atis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Atis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Atis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     Tweet  \\\n",
      "0          - awww, that's a bummer.  you shoulda got da...   \n",
      "1        is upset that he can't update his facebook by ...   \n",
      "2         i dived many times for the ball. managed to s...   \n",
      "3          my whole body feels itchy and like its on fire    \n",
      "4         no, it's not behaving at all. i'm mad. why am...   \n",
      "...                                                    ...   \n",
      "1599995  just woke up. having no school is the best fee...   \n",
      "1599996  thewdb.com - very cool to hear old walt interv...   \n",
      "1599997  are you ready for your mojo makeover? ask me f...   \n",
      "1599998  happy 38th birthday to my boo of alll time!!! ...   \n",
      "1599999                          happy #charitytuesday       \n",
      "\n",
      "                                           Processed_Tweet  \n",
      "0        awww that bummer shoulda got david carr third day  \n",
      "1        upset cant updat facebook text might cri resul...  \n",
      "2             dive mani time ball manag save rest go bound  \n",
      "3                          whole bodi feel itchi like fire  \n",
      "4                                    behav im mad cant see  \n",
      "...                                                    ...  \n",
      "1599995                         woke school best feel ever  \n",
      "1599996           thewdbcom cool hear old walt interview â  \n",
      "1599997                       readi mojo makeov ask detail  \n",
      "1599998  happi th birthday boo alll time tupac amaru sh...  \n",
      "1599999                               happi charitytuesday  \n",
      "\n",
      "[1600000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Assuming p_data is your DataFrame\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, numbers, and URLs\n",
    "    text = re.sub(r'http\\S+|@\\S+|\\d+|[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Rejoin tokens into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply the preprocess_text function to the 'Tweet' column\n",
    "p_data['Processed_Tweet'] = p_data['Tweet'].apply(preprocess_text)\n",
    "\n",
    "# Display the DataFrame with processed text\n",
    "print(p_data[['Tweet', 'Processed_Tweet']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW feature matrix shape: (1280000, 317230)\n",
      "TF-IDF feature matrix shape: (1280000, 317230)\n",
      "y_train shape: (1280000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(p_data['Processed_Tweet'], p_data['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Bag of Words (BoW) feature extraction\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# TF-IDF feature extraction\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# Display the shape of the resulting feature matrices\n",
    "print(\"BoW feature matrix shape:\", X_train_bow.shape)\n",
    "print(\"TF-IDF feature matrix shape:\", X_train_tfidf.shape)\n",
    "print(\"y_train shape:\",y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split - X_train_tfidf shape: (1280000, 317230)\n",
      "Before split - y_train shape: (1280000,)\n"
     ]
    }
   ],
   "source": [
    "# Check the size of your dataframes before the split\n",
    "print(\"Before split - X_train_tfidf shape:\", X_train_tfidf.shape)\n",
    "print(\"Before split - y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Supervised Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy (BoW): 0.75322265625\n",
      "Validation set accuracy (TF-IDF): 0.758984375\n",
      "Classification Report (BoW):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.75      5124\n",
      "           4       0.74      0.77      0.76      5116\n",
      "\n",
      "    accuracy                           0.75     10240\n",
      "   macro avg       0.75      0.75      0.75     10240\n",
      "weighted avg       0.75      0.75      0.75     10240\n",
      "\n",
      "Classification Report (TF-IDF):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75      5124\n",
      "           4       0.75      0.79      0.77      5116\n",
      "\n",
      "    accuracy                           0.76     10240\n",
      "   macro avg       0.76      0.76      0.76     10240\n",
      "weighted avg       0.76      0.76      0.76     10240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming X_train_bow, X_test_bow, X_train_tfidf, X_test_tfidf, y_train, y_test are available\n",
    "\n",
    "X_train_boww, _, y_train_lr, _ = train_test_split(X_train_bow, y_train, train_size=0.04, random_state=42)\n",
    "X_train_tfidff, _, y_train_lr, _ = train_test_split(X_train_tfidf, y_train, train_size=0.04, random_state=42)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "X_train_boww, X_val_bow_lr, y_train_bow_lr, y_val_lr = train_test_split(X_train_boww, y_train_lr, test_size=0.2, random_state=42)\n",
    "X_train_tfidff, X_val_tfidf_lr, y_train_tfidf_lr, y_val_lr = train_test_split(X_train_tfidff, y_train_lr, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression models\n",
    "lr_bow = LogisticRegression(random_state=42)\n",
    "lr_tfidf = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_bow.fit(X_train_boww, y_train_bow_lr)\n",
    "lr_tfidf.fit(X_train_tfidff, y_train_tfidf_lr)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val_bow_lr = lr_bow.predict(X_val_bow_lr)\n",
    "y_pred_val_tfidf_lr = lr_tfidf.predict(X_val_tfidf_lr)\n",
    "\n",
    "# Evaluate performance on the validation set\n",
    "accuracy_bow_lr = accuracy_score(y_val_lr, y_pred_val_bow_lr)\n",
    "accuracy_tfidf_lr = accuracy_score(y_val_lr, y_pred_val_tfidf_lr)\n",
    "\n",
    "print(\"Validation set accuracy (BoW):\", accuracy_bow_lr)\n",
    "print(\"Validation set accuracy (TF-IDF):\", accuracy_tfidf_lr)\n",
    "\n",
    "# Optionally, you can print other metrics like classification report\n",
    "print(\"Classification Report (BoW):\\n\", classification_report(y_val_lr, y_pred_val_bow_lr))\n",
    "print(\"Classification Report (TF-IDF):\\n\", classification_report(y_val_lr, y_pred_val_tfidf_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "30 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.6902832         nan 0.72614746        nan 0.74963379\n",
      "        nan 0.74714355        nan 0.73081055        nan 0.71782227]\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "30 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.7137207         nan 0.72043457        nan 0.7409668\n",
      "        nan 0.75405273        nan 0.74140625        nan 0.72009277]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for BoW: {'C': 0.1, 'penalty': 'l2'}\n",
      "Best hyperparameters for TF-IDF: {'C': 1, 'penalty': 'l2'}\n",
      "Validation set accuracy (BoW): 0.75400390625\n",
      "Validation set accuracy (TF-IDF): 0.758984375\n",
      "Classification Report (BoW):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.72      0.74      5124\n",
      "           4       0.74      0.79      0.76      5116\n",
      "\n",
      "    accuracy                           0.75     10240\n",
      "   macro avg       0.76      0.75      0.75     10240\n",
      "weighted avg       0.76      0.75      0.75     10240\n",
      "\n",
      "Classification Report (TF-IDF):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75      5124\n",
      "           4       0.75      0.79      0.77      5116\n",
      "\n",
      "    accuracy                           0.76     10240\n",
      "   macro avg       0.76      0.76      0.76     10240\n",
      "weighted avg       0.76      0.76      0.76     10240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Create Logistic Regression models\n",
    "lr_bow = LogisticRegression(random_state=42)\n",
    "lr_tfidf = LogisticRegression(random_state=42)\n",
    "\n",
    "# GridSearchCV for BoW\n",
    "grid_search_bow_lr= GridSearchCV(lr_bow, param_grid_lr, cv=5, scoring='accuracy')\n",
    "grid_search_bow_lr.fit(X_train_boww, y_train_bow_lr)\n",
    "\n",
    "# GridSearchCV for TF-IDF\n",
    "grid_search_tfidf_lr = GridSearchCV(lr_tfidf, param_grid_lr, cv=5, scoring='accuracy')\n",
    "grid_search_tfidf_lr.fit(X_train_tfidff, y_train_tfidf_lr)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params_bow_lr = grid_search_bow_lr.best_params_\n",
    "best_params_tfidf_lr = grid_search_tfidf_lr.best_params_\n",
    "\n",
    "# Train models with best hyperparameters\n",
    "best_lr_bow = LogisticRegression(**best_params_bow_lr, random_state=42)\n",
    "best_lr_tfidf = LogisticRegression(**best_params_tfidf_lr, random_state=42)\n",
    "\n",
    "best_lr_bow.fit(X_train_boww, y_train_bow_lr)\n",
    "best_lr_tfidf.fit(X_train_tfidff, y_train_tfidf_lr)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val_bow_lr = best_lr_bow.predict(X_val_bow_lr)\n",
    "y_pred_val_tfidf_lr = best_lr_tfidf.predict(X_val_tfidf_lr)\n",
    "\n",
    "# Evaluate performance on the validation set\n",
    "accuracy_bow_lr = accuracy_score(y_val_lr, y_pred_val_bow_lr)\n",
    "accuracy_tfidf_lr = accuracy_score(y_val_lr, y_pred_val_tfidf_lr)\n",
    "\n",
    "print(\"Best hyperparameters for BoW:\", best_params_bow_lr)\n",
    "print(\"Best hyperparameters for TF-IDF:\", best_params_tfidf_lr)\n",
    "\n",
    "print(\"Validation set accuracy (BoW):\", accuracy_bow_lr)\n",
    "print(\"Validation set accuracy (TF-IDF):\", accuracy_tfidf_lr)\n",
    "\n",
    "# Optionally, you can print other metrics like classification report\n",
    "print(\"Classification Report (BoW):\\n\", classification_report(y_val_lr, y_pred_val_bow_lr))\n",
    "print(\"Classification Report (TF-IDF):\\n\", classification_report(y_val_lr, y_pred_val_tfidf_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I'm gonna do it agian, this time using tranformer in `Feature Extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW feature matrix shape: (1600, 1000)\n",
      "TF-IDF feature matrix shape: (1600, 1000)\n",
      "y_train shape: (1600,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "total_samples = len(p_data['Processed_Tweet'])\n",
    "train_samples = int(0.01 * total_samples)\n",
    "test_samples = int(0.01 * total_samples)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(p_data['Processed_Tweet'], p_data['Label'], train_size=train_samples, test_size=test_samples, random_state=42)\n",
    "\n",
    "# Bag of Words (BoW) feature extraction\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# TF-IDF feature extraction\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# SelectKBest selects the top k features based on the chi-squared test\n",
    "k_best = 1000\n",
    "selector = SelectKBest(chi2, k=k_best)\n",
    "\n",
    "X_train_bow = selector.fit_transform(X_train_bow, y_train)\n",
    "X_test_bow = selector.transform(X_test_bow)\n",
    "\n",
    "X_train_tfidf = selector.fit_transform(X_train_tfidf, y_train)\n",
    "X_test_tfidf = selector.transform(X_test_tfidf)\n",
    "\n",
    "# Display the shape of the resulting feature matrices\n",
    "print(\"BoW feature matrix shape:\", X_train_bow.shape)\n",
    "print(\"TF-IDF feature matrix shape:\", X_train_tfidf.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy (BoW): 0.755859375\n",
      "Validation set accuracy (TF-IDF): 0.75390625\n",
      "Classification Report (BoW):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.71      0.74      1289\n",
      "           4       0.73      0.81      0.77      1271\n",
      "\n",
      "    accuracy                           0.76      2560\n",
      "   macro avg       0.76      0.76      0.76      2560\n",
      "weighted avg       0.76      0.76      0.76      2560\n",
      "\n",
      "Classification Report (TF-IDF):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74      1289\n",
      "           4       0.73      0.80      0.76      1271\n",
      "\n",
      "    accuracy                           0.75      2560\n",
      "   macro avg       0.76      0.75      0.75      2560\n",
      "weighted avg       0.76      0.75      0.75      2560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming X_train_bow, X_test_bow, X_train_tfidf, X_test_tfidf, y_train, y_test are available\n",
    "\n",
    "X_train_boww, _, y_train_lr, _ = train_test_split(X_train_bow, y_train, test_size=0.2, random_state=42)\n",
    "X_train_tfidff, _, y_train_lr, _ = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "X_train_boww, X_val_bow_lr, y_train_bow_lr, y_val_lr = train_test_split(X_train_boww, y_train_lr, test_size=0.2, random_state=42)\n",
    "X_train_tfidff, X_val_tfidf_lr, y_train_tfidf_lr, y_val_lr = train_test_split(X_train_tfidff, y_train_lr, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression models\n",
    "lr_bow = LogisticRegression(random_state=42)\n",
    "lr_tfidf = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_bow.fit(X_train_boww, y_train_bow_lr)\n",
    "lr_tfidf.fit(X_train_tfidff, y_train_tfidf_lr)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val_bow_lr = lr_bow.predict(X_val_bow_lr)\n",
    "y_pred_val_tfidf_lr = lr_tfidf.predict(X_val_tfidf_lr)\n",
    "\n",
    "# Evaluate performance on the validation set\n",
    "accuracy_bow_lr = accuracy_score(y_val_lr, y_pred_val_bow_lr)\n",
    "accuracy_tfidf_lr = accuracy_score(y_val_lr, y_pred_val_tfidf_lr)\n",
    "\n",
    "print(\"Validation set accuracy (BoW):\", accuracy_bow_lr)\n",
    "print(\"Validation set accuracy (TF-IDF):\", accuracy_tfidf_lr)\n",
    "\n",
    "# Optionally, you can print other metrics like classification report\n",
    "print(\"Classification Report (BoW):\\n\", classification_report(y_val_lr, y_pred_val_bow_lr))\n",
    "print(\"Classification Report (TF-IDF):\\n\", classification_report(y_val_lr, y_pred_val_tfidf_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "30 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.690625          nan 0.70693359        nan 0.74414062\n",
      "        nan 0.75751953        nan 0.75849609        nan 0.75556641]\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "30 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.54667969        nan 0.70957031        nan 0.7328125\n",
      "        nan 0.75683594        nan 0.76142578        nan 0.76201172]\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for BoW: {'C': 10, 'penalty': 'l2'}\n",
      "Best hyperparameters for TF-IDF: {'C': 100, 'penalty': 'l2'}\n",
      "Validation set accuracy (BoW): 0.76328125\n",
      "Validation set accuracy (TF-IDF): 0.7671875\n",
      "Classification Report (BoW):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75      1289\n",
      "           4       0.74      0.81      0.77      1271\n",
      "\n",
      "    accuracy                           0.76      2560\n",
      "   macro avg       0.77      0.76      0.76      2560\n",
      "weighted avg       0.77      0.76      0.76      2560\n",
      "\n",
      "Classification Report (TF-IDF):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75      1289\n",
      "           4       0.74      0.82      0.78      1271\n",
      "\n",
      "    accuracy                           0.77      2560\n",
      "   macro avg       0.77      0.77      0.77      2560\n",
      "weighted avg       0.77      0.77      0.77      2560\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Create Logistic Regression models\n",
    "lr_bow = LogisticRegression(random_state=42)\n",
    "lr_tfidf = LogisticRegression(random_state=42)\n",
    "\n",
    "# GridSearchCV for BoW\n",
    "grid_search_bow_lr= GridSearchCV(lr_bow, param_grid_lr, cv=5, scoring='accuracy')\n",
    "grid_search_bow_lr.fit(X_train_boww, y_train_bow_lr)\n",
    "\n",
    "# GridSearchCV for TF-IDF\n",
    "grid_search_tfidf_lr = GridSearchCV(lr_tfidf, param_grid_lr, cv=5, scoring='accuracy')\n",
    "grid_search_tfidf_lr.fit(X_train_tfidff, y_train_tfidf_lr)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params_bow_lr = grid_search_bow_lr.best_params_\n",
    "best_params_tfidf_lr = grid_search_tfidf_lr.best_params_\n",
    "\n",
    "# Train models with best hyperparameters\n",
    "best_lr_bow = LogisticRegression(**best_params_bow_lr, random_state=42)\n",
    "best_lr_tfidf = LogisticRegression(**best_params_tfidf_lr, random_state=42)\n",
    "\n",
    "best_lr_bow.fit(X_train_boww, y_train_bow_lr)\n",
    "best_lr_tfidf.fit(X_train_tfidff, y_train_tfidf_lr)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val_bow_lr = best_lr_bow.predict(X_val_bow_lr)\n",
    "y_pred_val_tfidf_lr = best_lr_tfidf.predict(X_val_tfidf_lr)\n",
    "\n",
    "# Evaluate performance on the validation set\n",
    "accuracy_bow_lr = accuracy_score(y_val_lr, y_pred_val_bow_lr)\n",
    "accuracy_tfidf_lr = accuracy_score(y_val_lr, y_pred_val_tfidf_lr)\n",
    "\n",
    "print(\"Best hyperparameters for BoW:\", best_params_bow_lr)\n",
    "print(\"Best hyperparameters for TF-IDF:\", best_params_tfidf_lr)\n",
    "\n",
    "print(\"Validation set accuracy (BoW):\", accuracy_bow_lr)\n",
    "print(\"Validation set accuracy (TF-IDF):\", accuracy_tfidf_lr)\n",
    "\n",
    "# Optionally, you can print other metrics like classification report\n",
    "print(\"Classification Report (BoW):\\n\", classification_report(y_val_lr, y_pred_val_bow_lr))\n",
    "print(\"Classification Report (TF-IDF):\\n\", classification_report(y_val_lr, y_pred_val_tfidf_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `As we can see, it becomes slightly better`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy (BoW - Random Forest): 0.724609375\n",
      "Validation set accuracy (TF-IDF - Random Forest): 0.71484375\n",
      "Classification Report (BoW - Random Forest):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72      1289\n",
      "           4       0.71      0.75      0.73      1271\n",
      "\n",
      "    accuracy                           0.72      2560\n",
      "   macro avg       0.73      0.72      0.72      2560\n",
      "weighted avg       0.73      0.72      0.72      2560\n",
      "\n",
      "Classification Report (TF-IDF - Random Forest):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.68      0.70      1289\n",
      "           4       0.70      0.75      0.72      1271\n",
      "\n",
      "    accuracy                           0.71      2560\n",
      "   macro avg       0.72      0.72      0.71      2560\n",
      "weighted avg       0.72      0.71      0.71      2560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming X_train_bow, X_val_bow, X_test_bow, X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val are available\n",
    "\n",
    "X_train_bow_rf, _, y_train_rf, _ = train_test_split(X_train_bow, y_train, test_size=0.2, random_state=42)\n",
    "X_train_tfidf_rf, _, y_train_rf, _ = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "X_train_bow_rf, X_val_bow_rf, y_train_bow_rf, y_val_rf = train_test_split(X_train_bow_rf, y_train_rf, test_size=0.2, random_state=42)\n",
    "X_train_tfidf_rf, X_val_tfidf_rf, y_train_tfidf_rf, y_val_rf = train_test_split(X_train_tfidf_rf, y_train_rf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest models\n",
    "rf_bow = RandomForestClassifier(random_state=42)\n",
    "rf_tfidf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_bow.fit(X_train_bow_rf, y_train_bow_rf)\n",
    "rf_tfidf.fit(X_train_tfidf_rf, y_train_tfidf_rf)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val_bow_rf = rf_bow.predict(X_val_bow_rf)\n",
    "y_pred_val_tfidf_rf = rf_tfidf.predict(X_val_tfidf_rf)\n",
    "\n",
    "# Evaluate performance on the validation set\n",
    "accuracy_bow_rf= accuracy_score(y_val_rf, y_pred_val_bow_rf)\n",
    "accuracy_tfidf_rf = accuracy_score(y_val_rf, y_pred_val_tfidf_rf)\n",
    "\n",
    "print(\"Validation set accuracy (BoW - Random Forest):\", accuracy_bow_rf)\n",
    "print(\"Validation set accuracy (TF-IDF - Random Forest):\", accuracy_tfidf_rf)\n",
    "\n",
    "# Optionally, you can print other metrics like classification report\n",
    "print(\"Classification Report (BoW - Random Forest):\\n\", classification_report(y_val_rf, y_pred_val_bow_rf))\n",
    "print(\"Classification Report (TF-IDF - Random Forest):\\n\", classification_report(y_val_rf, y_pred_val_tfidf_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for BoW: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best Hyperparameters for TF-IDF: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Validation set accuracy (BoW - Random Forest - Best Hyperparameters): 0.7296875\n",
      "Validation set accuracy (TF-IDF - Random Forest - Best Hyperparameters): 0.721875\n",
      "Classification Report (BoW - Random Forest - Best Hyperparameters):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73      1289\n",
      "           4       0.72      0.74      0.73      1271\n",
      "\n",
      "    accuracy                           0.73      2560\n",
      "   macro avg       0.73      0.73      0.73      2560\n",
      "weighted avg       0.73      0.73      0.73      2560\n",
      "\n",
      "Classification Report (TF-IDF - Random Forest - Best Hyperparameters):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.69      0.71      1289\n",
      "           4       0.70      0.76      0.73      1271\n",
      "\n",
      "    accuracy                           0.72      2560\n",
      "   macro avg       0.72      0.72      0.72      2560\n",
      "weighted avg       0.72      0.72      0.72      2560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter grid for RandomForestClassifier\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV for BoW\n",
    "grid_search_bow_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_bow_rf.fit(X_train_bow_rf, y_train_bow_rf)\n",
    "\n",
    "# Best hyperparameters for BoW\n",
    "best_params_bow_rf = grid_search_bow_rf.best_params_\n",
    "print(\"Best Hyperparameters for BoW:\", best_params_bow_rf)\n",
    "\n",
    "# Train Random Forest with best hyperparameters on BoW\n",
    "rf_bow_best = RandomForestClassifier(random_state=42, **best_params_bow_rf)\n",
    "rf_bow_best.fit(X_train_bow_rf, y_train_bow_rf)\n",
    "\n",
    "# GridSearchCV for TF-IDF\n",
    "grid_search_tfidf_rf= GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_tfidf_rf.fit(X_train_tfidf_rf, y_train_tfidf_rf)\n",
    "\n",
    "# Best hyperparameters for TF-IDF\n",
    "best_params_tfidf_rf = grid_search_tfidf_rf.best_params_\n",
    "print(\"Best Hyperparameters for TF-IDF:\", best_params_tfidf_rf)\n",
    "\n",
    "# Train Random Forest with best hyperparameters on TF-IDF\n",
    "rf_tfidf_best = RandomForestClassifier(random_state=42, **best_params_tfidf_rf)\n",
    "rf_tfidf_best.fit(X_train_tfidf_rf, y_train_tfidf_rf)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val_bow_rf_best = rf_bow_best.predict(X_val_bow_rf)\n",
    "y_pred_val_tfidf_rf_best = rf_tfidf_best.predict(X_val_tfidf_rf)\n",
    "\n",
    "# Evaluate performance on the validation set with best hyperparameters\n",
    "accuracy_bow_rf_best = accuracy_score(y_val_rf, y_pred_val_bow_rf_best)\n",
    "accuracy_tfidf_rf_best = accuracy_score(y_val_rf, y_pred_val_tfidf_rf_best)\n",
    "\n",
    "print(\"Validation set accuracy (BoW - Random Forest - Best Hyperparameters):\", accuracy_bow_rf_best)\n",
    "print(\"Validation set accuracy (TF-IDF - Random Forest - Best Hyperparameters):\", accuracy_tfidf_rf_best)\n",
    "\n",
    "# Optionally, you can print other metrics like classification report\n",
    "print(\"Classification Report (BoW - Random Forest - Best Hyperparameters):\\n\", classification_report(y_val_rf, y_pred_val_bow_rf_best))\n",
    "print(\"Classification Report (TF-IDF - Random Forest - Best Hyperparameters):\\n\", classification_report(y_val_rf, y_pred_val_tfidf_rf_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy (BoW - SVM): 0.74609375\n",
      "Validation set accuracy (TF-IDF - SVM): 0.762109375\n",
      "Classification Report (BoW - SVM):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.72      0.74      1289\n",
      "           4       0.73      0.78      0.75      1271\n",
      "\n",
      "    accuracy                           0.75      2560\n",
      "   macro avg       0.75      0.75      0.75      2560\n",
      "weighted avg       0.75      0.75      0.75      2560\n",
      "\n",
      "Classification Report (TF-IDF - SVM):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75      1289\n",
      "           4       0.73      0.82      0.77      1271\n",
      "\n",
      "    accuracy                           0.76      2560\n",
      "   macro avg       0.77      0.76      0.76      2560\n",
      "weighted avg       0.77      0.76      0.76      2560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming X_train_bow, X_val_bow, X_test_bow, X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val are available\n",
    "\n",
    "X_train_bow_svm, _, y_train_svm, _ = train_test_split(X_train_bow, y_train, test_size=0.2, random_state=42)\n",
    "X_train_tfidf_svm, _, y_train_svm, _ = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "X_train_bow_svm, X_val_bow_svm, y_train_bow_svm, y_val_svm = train_test_split(X_train_bow_svm, y_train_svm, test_size=0.2, random_state=42)\n",
    "X_train_tfidf_svm, X_val_tfidf_svm, y_train_tfidf_svm, y_val_svm = train_test_split(X_train_tfidf_svm, y_train_svm, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Support Vector Machine models\n",
    "svm_bow = SVC(random_state=42)\n",
    "svm_tfidf = SVC(random_state=42)\n",
    "\n",
    "svm_bow.fit(X_train_bow_svm, y_train_bow_svm)\n",
    "svm_tfidf.fit(X_train_tfidf_svm, y_train_tfidf_svm)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val_bow_svm = svm_bow.predict(X_val_bow_svm)\n",
    "y_pred_val_tfidf_svm = svm_tfidf.predict(X_val_tfidf_svm)\n",
    "\n",
    "# Evaluate performance on the validation set\n",
    "accuracy_bow_svm = accuracy_score(y_val_svm, y_pred_val_bow_svm)\n",
    "accuracy_tfidf_svm = accuracy_score(y_val_svm, y_pred_val_tfidf_svm)\n",
    "\n",
    "print(\"Validation set accuracy (BoW - SVM):\", accuracy_bow_svm)\n",
    "print(\"Validation set accuracy (TF-IDF - SVM):\", accuracy_tfidf_svm)\n",
    "\n",
    "# Optionally, you can print other metrics like classification report\n",
    "print(\"Classification Report (BoW - SVM):\\n\", classification_report(y_val_svm, y_pred_val_bow_svm))\n",
    "print(\"Classification Report (TF-IDF - SVM):\\n\", classification_report(y_val_svm, y_pred_val_tfidf_svm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy (BoW - SVM - Best): 0.75546875\n",
      "Validation set accuracy (TF-IDF - SVM - Best): 0.762109375\n",
      "Best hyperparameters (BoW - SVM): {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best hyperparameters (TF-IDF - SVM): {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning for the SVM with BoW\n",
    "svm_bow_tuned = SVC(random_state=42)\n",
    "grid_search_bow_svm = GridSearchCV(svm_bow_tuned, param_grid_svm, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_bow_svm.fit(X_train_bow_svm, y_train_bow_svm)\n",
    "\n",
    "# Get the best hyperparameters for BoW\n",
    "best_params_bow_svm = grid_search_bow_svm.best_params_\n",
    "\n",
    "# Train the SVM with BoW using the best hyperparameters\n",
    "svm_bow_best = SVC(**best_params_bow_svm, random_state=42)\n",
    "svm_bow_best.fit(X_train_bow_svm, y_train_bow_svm)\n",
    "\n",
    "# Perform hyperparameter tuning for the SVM with TF-IDF\n",
    "svm_tfidf_tuned = SVC(random_state=42)\n",
    "grid_search_tfidf_svm = GridSearchCV(svm_tfidf_tuned, param_grid_svm, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_tfidf_svm.fit(X_train_tfidf_svm, y_train_tfidf_svm)\n",
    "\n",
    "# Get the best hyperparameters for TF-IDF\n",
    "best_params_tfidf_svm = grid_search_tfidf_svm.best_params_\n",
    "\n",
    "# Train the SVM with TF-IDF using the best hyperparameters\n",
    "svm_tfidf_best = SVC(**best_params_tfidf_svm, random_state=42)\n",
    "svm_tfidf_best.fit(X_train_tfidf_svm, y_train_tfidf_svm)\n",
    "\n",
    "# Predict on the validation set with the best models\n",
    "y_pred_val_bow_best_svm = svm_bow_best.predict(X_val_bow_svm)\n",
    "y_pred_val_tfidf_best_svm = svm_tfidf_best.predict(X_val_tfidf_svm)\n",
    "\n",
    "# Evaluate performance on the validation set with the best models\n",
    "accuracy_bow_best_svm = accuracy_score(y_val_svm, y_pred_val_bow_best_svm)\n",
    "accuracy_tfidf_best_svm = accuracy_score(y_val_svm, y_pred_val_tfidf_best_svm)\n",
    "\n",
    "print(\"Validation set accuracy (BoW - SVM - Best):\", accuracy_bow_best_svm)\n",
    "print(\"Validation set accuracy (TF-IDF - SVM - Best):\", accuracy_tfidf_best_svm)\n",
    "\n",
    "# Optionally, you can print the best hyperparameters\n",
    "print(\"Best hyperparameters (BoW - SVM):\", best_params_bow_svm)\n",
    "print(\"Best hyperparameters (TF-IDF - SVM):\", best_params_tfidf_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: TF-IDF - SVM\n",
      "Best Accuracy: 0.762109375\n"
     ]
    }
   ],
   "source": [
    "# Compare accuracy scores and choose the best model\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Compare accuracy for BoW SVM\n",
    "if accuracy_bow_best_svm > best_accuracy:\n",
    "    best_model = \"BoW - SVM\"\n",
    "    best_accuracy = accuracy_bow_best_svm\n",
    "\n",
    "# Compare accuracy for TF-IDF SVM\n",
    "if accuracy_tfidf_best_svm > best_accuracy:\n",
    "    best_model = \"TF-IDF - SVM\"\n",
    "    best_accuracy = accuracy_tfidf_best_svm\n",
    "\n",
    "# Print the best model and its accuracy\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Although logistic regression seems the best model we got, but on more data TF-IDF SVM was the best one we got. I have test that, but I cannot show it here, cause I mistakely delete it and it will take about one and a half hour to do that agian. So, we're gonna continue with this model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Using Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 50/50 [07:42<00:00,  9.24s/it, loss=0.678]\n",
      "Epoch 2: 100%|██████████| 50/50 [09:28<00:00, 11.38s/it, loss=0.559]\n",
      "Epoch 3: 100%|██████████| 50/50 [11:32<00:00, 13.84s/it, loss=0.395] \n",
      "Evaluation: 100%|██████████| 50/50 [03:58<00:00,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Map labels to binary format (0 and 1)\n",
    "y_train_binary = y_train.map({0: 0, 4: 1})\n",
    "y_test_binary = y_test.map({0: 0, 4: 1})\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize input text\n",
    "max_len = 128  # Maximum sequence length\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    X_train.tolist(),\n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    X_test.tolist(),\n",
    "    max_length=max_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# Convert tokenized inputs to PyTorch tensors\n",
    "train_input_ids = torch.tensor(tokens_train['input_ids'])\n",
    "train_attention_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_labels = torch.tensor(y_train_binary.tolist())\n",
    "\n",
    "test_input_ids = torch.tensor(tokens_test['input_ids'])\n",
    "test_attention_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_labels = torch.tensor(y_test_binary.tolist())\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)\n",
    "\n",
    "# Fine-tune the BERT model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    tqdm_dataloader = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in tqdm_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tqdm_dataloader.set_postfix({'loss': total_loss / len(train_dataloader)})\n",
    "        \n",
    "# Evaluation\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluation\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1]}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        preds.extend(logits.argmax(dim=1).tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_binary, preds)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Fine-tune Using Hugging Face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1:   0%|          | 0/200 [00:00<?, ?it/s]C:\\Users\\Atis\\AppData\\Local\\Temp\\ipykernel_1812\\1286543125.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n",
      "Epoch 1: 100%|██████████| 200/200 [07:00<00:00,  2.10s/it]\n",
      "Epoch 2: 100%|██████████| 200/200 [06:39<00:00,  2.00s/it]\n",
      "Epoch 3: 100%|██████████| 200/200 [09:18<00:00,  2.79s/it]\n",
      "100%|██████████| 200/200 [02:03<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.30875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Map labels to binary format (0 and 1)\n",
    "y_train_binary = y_train.map({0: 0, 4: 1})\n",
    "y_test_binary = y_test.map({0: 0, 4: 1})\n",
    "\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenization\n",
    "max_length = 128  # Adjust according to your data\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=max_length)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(y_train_binary.values)\n",
    "test_labels = torch.tensor(y_test_binary.values)\n",
    "\n",
    "# Dataset preparation\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "# Fine-tuning\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "epochs = 3  # Adjust as needed\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in tqdm(torch.utils.data.DataLoader(test_dataset, batch_size=8)):\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### حقیقتا نرسیدم بلایی سر این مدل یا مدل برت بیارم و تیونشون کنم. از اینجا به بعد فقط سعی کردم تا جایی که میتونم خالی نذارم :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('D:\\Data Science\\Project\\Data\\\\tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "companies = pd.read_csv('D:\\Data Science\\Project\\Data\\\\companies.csv')\n",
    "\n",
    "# Assuming your dataset is stored in a DataFrame called 'tweets'\n",
    "stock_mentions = tweets['text'].str.extractall(r'\\$([a-zA-Z]+)').stack().reset_index(drop=True)\n",
    "\n",
    "# Count the frequency of each stock mention\n",
    "stock_counts = stock_mentions.value_counts()\n",
    "\n",
    "# Create a new column 'frequency' in the 'companies' DataFrame\n",
    "companies['frequency'] = companies['ticker'].map(stock_counts).fillna(0).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tweets['cashtags'] = ''\n",
    "\n",
    "# Define a function to extract cashtags from the text\n",
    "def extract_cashtags(text):\n",
    "    cashtags = re.findall(r'\\$\\w+', text)\n",
    "    return cashtags\n",
    "\n",
    "# Apply the function to the 'text' column and store the results in the 'cashtags' column\n",
    "tweets['cashtags'] = tweets['text'].apply(extract_cashtags)\n",
    "tweets['cashtags'] = tweets['cashtags'].apply(lambda x: [tag.upper() for tag in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'cashtags_num' with the number of cashtags in each row\n",
    "tweets['cashtags_num'] = tweets['cashtags'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove '$' from cashtags\n",
    "def remove_dollar_sign(cashtags):\n",
    "    return [tag.replace('$', '') for tag in cashtags]\n",
    "\n",
    "tweets['cashtags'] = tweets['cashtags'].apply(remove_dollar_sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning\n",
    "import re\n",
    "\n",
    "tweets['text'] = tweets['text'].apply(lambda x: re.sub(r'http\\S+|@\\S+', '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Atis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Atis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Atis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      text  \\\n",
      "0        RT  To all the weak hands who sold their $ltc ...   \n",
      "1        Investors Eye Fed, But Bond ETFs Still Add Ass...   \n",
      "2        Sell $NLNK (Ne❑wLink Genetics Corporation) tha...   \n",
      "3        Increase: $VCO $NTES $BPT $ENIC $QIWI $JP $STX...   \n",
      "4        Former #FDA commissioner Califf joins Verily, ...   \n",
      "...                                                    ...   \n",
      "9091534  Contrasting Pure Cycle Corporation $PCYO &amp;...   \n",
      "9091535  Contrasting Harvard Bioscience $HBIO &amp; Ful...   \n",
      "9091536  NRG Energy, Inc. $NRG Receives $21.13 Average ...   \n",
      "9091537  Reviewing Arch Therapeutics $ARTH and Globus M...   \n",
      "9091538  $JD Beginning stage of a breakout. Strong buy ...   \n",
      "\n",
      "                                           Processed_Tweet  \n",
      "0        rt weak hand sold ltc short pour one tonight n...  \n",
      "1        investor eye fed bond etf still add asset ief tlt  \n",
      "2                  sell nlnk newlink genet corpor thank el  \n",
      "3        increas vco nte bpt enic qiwi jp stx dl lrcx s...  \n",
      "4        former fda commission califf join verili help ...  \n",
      "...                                                    ...  \n",
      "9091534  contrast pure cycl corpor pcyo amp companhia d...  \n",
      "9091535  contrast harvard bioscienc hbio amp fulgent ge...  \n",
      "9091536  nrg energi inc nrg receiv averag target price ...  \n",
      "9091537        review arch therapeut arth globu medic gmed  \n",
      "9091538   jd begin stage breakout strong buy rate barchart  \n",
      "\n",
      "[9091539 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, numbers, and URLs\n",
    "    text = re.sub(r'http\\S+|@\\S+|\\d+|[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Rejoin tokens into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply the preprocess_text function to the 'Tweet' column\n",
    "tweets['Processed_Tweet'] = tweets['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the DataFrame with processed text\n",
    "print(tweets[['text', 'Processed_Tweet']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying TF-IDF SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a smaller subset of your data\n",
    "sample_size = 5000\n",
    "tweets_sample = tweets['Processed_Tweet'].sample(n=sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# TF-IDF feature extraction\n",
    "X_sample_tfidf = tfidf_vectorizer.transform(tweets_sample)\n",
    "\n",
    "# Select the top k features based on the chi-squared test\n",
    "X_sample_tfidf = selector.transform(X_sample_tfidf)\n",
    "\n",
    "# Predict sentiments on the sampled dataset\n",
    "y_pred_sample = svm_tfidf.predict(X_sample_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# TF-IDF feature extraction\n",
    "X_sample_tfidf_lr = tfidf_vectorizer.transform(tweets_sample)\n",
    "\n",
    "# Select the top k features based on the chi-squared test\n",
    "X_sample_tfidf_lr = selector.transform(X_sample_tfidf_lr)\n",
    "\n",
    "# Predict sentiments on the sampled dataset\n",
    "y_pred_sample_lr = best_lr_tfidf.predict(X_sample_tfidf_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4385281    rt unh wichmann unh gener strong cash flow yoy...\n",
       "1425403       nice shake weak hand yesterday mm onto mofoznk\n",
       "2171408    rt vdrm cnbx owcp usrm aapl fb goog stock znga...\n",
       "3564549       commerci metal compani director pick share cmc\n",
       "7335060    hunt stock short here one w low piotroski scor...\n",
       "                                 ...                        \n",
       "8666375    didnt think would work got day copi trader taa...\n",
       "3742177     rt one look good eth faucet gt un bon eth faucet\n",
       "8430077    ixi introduc pvxkvex pul gener scientif rampd ...\n",
       "7836765                                       rt cac pt done\n",
       "5818043      cbpo china biolog product inc sec file form aba\n",
       "Name: Processed_Tweet, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>retweeted_user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>cashtags</th>\n",
       "      <th>cashtags_num</th>\n",
       "      <th>Processed_Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>865326129644797957</td>\n",
       "      <td>RT  To all the weak hands who sold their $ltc ...</td>\n",
       "      <td>859451814940336128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>865323140531896320</td>\n",
       "      <td>3005609114</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Thu May 18 22:00:00 +0000 2017</td>\n",
       "      <td>[LTC]</td>\n",
       "      <td>1</td>\n",
       "      <td>rt weak hand sold ltc short pour one tonight n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>865326133008642049</td>\n",
       "      <td>Investors Eye Fed, But Bond ETFs Still Add Ass...</td>\n",
       "      <td>44060322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>Thu May 18 22:00:01 +0000 2017</td>\n",
       "      <td>[IEF, TLT]</td>\n",
       "      <td>2</td>\n",
       "      <td>investor eye fed bond etf still add asset ief tlt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  865326129644797957  RT  To all the weak hands who sold their $ltc ...   \n",
       "1  865326133008642049  Investors Eye Fed, But Bond ETFs Still Add Ass...   \n",
       "\n",
       "              user_id  in_reply_to_status_id  in_reply_to_user_id  \\\n",
       "0  859451814940336128                      0                    0   \n",
       "1            44060322                      0                    0   \n",
       "\n",
       "   retweeted_status_id  retweeted_user_id lang  \\\n",
       "0   865323140531896320         3005609114   en   \n",
       "1                    0                  0   en   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"https://about.twitter.com/products/tw...   \n",
       "\n",
       "                       created_at    cashtags  cashtags_num  \\\n",
       "0  Thu May 18 22:00:00 +0000 2017       [LTC]             1   \n",
       "1  Thu May 18 22:00:01 +0000 2017  [IEF, TLT]             2   \n",
       "\n",
       "                                     Processed_Tweet  \n",
       "0  rt weak hand sold ltc short pour one tonight n...  \n",
       "1  investor eye fed bond etf still add asset ief tlt  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>name</th>\n",
       "      <th>exchange</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>1.915000e+10</td>\n",
       "      <td>17189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>Alcoa Corp</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>6.480000e+09</td>\n",
       "      <td>5512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker                  name exchange  capitalization  frequency\n",
       "0      A  Agilent Technologies     NYSE    1.915000e+10      17189\n",
       "1     AA            Alcoa Corp     NYSE    6.480000e+09       5512"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Correlation between sentiments of each CashTag and its value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'], format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "# Merge sentiment data with financial data based on CashTag and time interval\n",
    "merged_data = pd.merge(sentiment_data, financial_data, on=['cashtags', 'created_at'])\n",
    "\n",
    "# Define function to calculate correlation with different measures\n",
    "def calculate_correlation(data, sentiment_col, value_col):\n",
    "    pearson_corr, _ = pearsonr(data[sentiment_col], data[value_col])\n",
    "    spearman_corr, _ = spearmanr(data[sentiment_col], data[value_col])\n",
    "    return pearson_corr, spearman_corr\n",
    "\n",
    "# Calculate correlation for each CashTag\n",
    "correlation_results = {}\n",
    "for cash_tag, group in merged_data.groupby('CashTag'):\n",
    "    pearson_corr, spearman_corr = calculate_correlation(group, 'SentimentScore', 'Value')\n",
    "    correlation_results[cash_tag] = {'Pearson': pearson_corr, 'Spearman': spearman_corr}\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "correlation_df = pd.DataFrame(correlation_results).T\n",
    "\n",
    "# Optionally, visualize the correlation\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_df, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation between Sentiment and Value for Each CashTag')\n",
    "plt.xlabel('Correlation Measure')\n",
    "plt.ylabel('CashTag')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
